{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dependencies"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Python : 3.6.5\n",
    "Tensorflow : 1.10.1\n",
    "Keras : 2.2.2\n",
    "Numpy : 1.15.4\n",
    "OpenCV : 3.4.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T16:15:28.646668Z",
     "start_time": "2018-12-06T16:15:28.638420Z"
    }
   },
   "outputs": [],
   "source": [
    "path = '/Users/maelfabien/filrouge_pole_emploi/Common/'\n",
    "local_path = '/Users/maelfabien/Desktop/LocalDB/Videos/'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initial steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T16:15:31.910138Z",
     "start_time": "2018-12-06T16:15:28.650589Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n",
      "/anaconda3/lib/python3.6/site-packages/requests/__init__.py:80: RequestsDependencyWarning: urllib3 (1.21.1) or chardet (2.3.0) doesn't match a supported version!\n",
      "  RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "#Imports\n",
    "import cv2\n",
    "import numpy as np\n",
    "from scipy import ndimage\n",
    "from keras.preprocessing.image import ImageDataGenerator, array_to_img, img_to_array, load_img\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from keras.callbacks import TensorBoard\n",
    "from time import time\n",
    "from time import sleep\n",
    "from keras.models import model_from_json\n",
    "from keras.optimizers import SGD\n",
    "from scipy.ndimage import zoom\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T16:15:31.917850Z",
     "start_time": "2018-12-06T16:15:31.913569Z"
    }
   },
   "outputs": [],
   "source": [
    "emotions = {1 : 'Neutral', 2 : 'calm', 3 : 'happy', 4 : 'sad', 5 : 'angry', 6 : 'fearful', 7 : 'disgust', 8 : 'surprised'}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T16:15:31.939299Z",
     "start_time": "2018-12-06T16:15:31.930037Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working with 9 videos\n"
     ]
    }
   ],
   "source": [
    "#Grab folder\n",
    "folder = local_path + 'Datas/Actor_01/'\n",
    "onlyfiles = [f for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))]\n",
    "print(\"Working with {0} videos\".format(len(onlyfiles)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Video facial emotion recognition using Convolution Neural Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create train set from video"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T16:16:24.374824Z",
     "start_time": "2018-12-06T16:16:24.368734Z"
    }
   },
   "outputs": [],
   "source": [
    "shape_x = 225\n",
    "shape_y = 135"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T15:01:02.369770Z",
     "start_time": "2018-12-06T15:01:02.357120Z"
    }
   },
   "outputs": [],
   "source": [
    "def detect_face(frame):\n",
    "    #Modèle cascade classifier\n",
    "    cascPath = local_path + '/models/haarcascade_frontalface_default.xml'\n",
    "    faceCascade = cv2.CascadeClassifier(cascPath)\n",
    "    \n",
    "    #BGR -> Gray conversion\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    \n",
    "    #Cascade MultiScale classifier\n",
    "    detected_faces = faceCascade.detectMultiScale(\n",
    "                                                  gray,\n",
    "                                                  scaleFactor=1.1,\n",
    "                                                  minNeighbors=6,\n",
    "                                                  minSize=(shape_x, shape_y),\n",
    "                                                  flags=cv2.CASCADE_SCALE_IMAGE\n",
    "                                                  )\n",
    "                                                  \n",
    "    return gray, detected_faces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T15:01:07.053137Z",
     "start_time": "2018-12-06T15:01:07.039767Z"
    }
   },
   "outputs": [],
   "source": [
    "#Extraire les features faciales\n",
    "def extract_face_features(gray, detected_face, offset_coefficients):\n",
    "    \n",
    "    #Region dans laquelle la face est détectée\n",
    "    (x, y, w, h) = detected_face\n",
    "    #X et y correspondent à la conversion en gris par gray, et w, h correspondent à la hauteur/largeur\n",
    "\n",
    "    \n",
    "    #Offset coefficient, np.floor takes the lowest integer (delete border of the image)\n",
    "    horizontal_offset = np.int(np.floor(offset_coefficients[0] * w))\n",
    "    vertical_offset = np.int(np.floor(offset_coefficients[1] * h))\n",
    "\n",
    "    #gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    #gray transforme l'image\n",
    "    extracted_face = gray[y+vertical_offset:y+h, x+horizontal_offset:x-horizontal_offset+w]\n",
    "    \n",
    "    #Zoom sur la face extraite\n",
    "    new_extracted_face = zoom(extracted_face, (shape_x / extracted_face.shape[0],shape_y / extracted_face.shape[1]))\n",
    "    #cast type float\n",
    "    new_extracted_face = new_extracted_face.astype(np.float32)\n",
    "    #scale\n",
    "    new_extracted_face /= float(new_extracted_face.max())\n",
    "    #print(new_extracted_face)\n",
    "    return new_extracted_face"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For 1 folder of datas :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T15:42:20.527357Z",
     "start_time": "2018-12-06T15:39:55.399683Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/scipy/ndimage/interpolation.py:583: UserWarning: From scipy 0.13.0, the output shape of zoom() is calculated with round() instead of int() - for these inputs the size of the returned array has changed.\n",
      "  \"the returned array has changed.\", UserWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video : 1\n",
      "Video : 2\n",
      "Video : 3\n",
      "Video : 4\n",
      "Video : 5\n",
      "Video : 6\n",
      "Video : 7\n",
      "Video : 8\n",
      "Video : 9\n"
     ]
    }
   ],
   "source": [
    "#Output in a .npy file\n",
    "dataset = np.empty((0, shape_x*shape_y + 1))\n",
    "\n",
    "j=1\n",
    "i=1\n",
    "k=1\n",
    "\n",
    "for vid in onlyfiles :\n",
    "    if (vid[0:2] != '02') :\n",
    "\n",
    "        vidcap = cv2.VideoCapture(folder + vid)\n",
    "        #success,img = vidcap.read()\n",
    "        success = True\n",
    "    \n",
    "        while success :\n",
    "\n",
    "            success,img = vidcap.read()\n",
    "            if not success :\n",
    "                break\n",
    "\n",
    "            gray, detected_faces = detect_face(img)\n",
    "        \n",
    "            for face in detected_faces :\n",
    "            \n",
    "                extracted_face = extract_face_features(gray, face, (0.075, 0.05)) #(0.075, 0.05)\n",
    "\n",
    "                extracted_face = np.append(extracted_face.flatten(), int(vid[6:8]))\n",
    "                dataset = np.vstack((dataset, extracted_face))\n",
    "\n",
    "            i = i + 1\n",
    "        \n",
    "        print('Video : '+ str(j))  \n",
    "    \n",
    "        j = j + 1\n",
    "\n",
    "np.save(local_path + 'savedmodels/datas', dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the whole repository :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T12:55:44.410388Z",
     "start_time": "2018-11-29T12:55:44.393059Z"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "EOL while scanning string literal (<ipython-input-17-c699a2be17e1>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-17-c699a2be17e1>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    folder = path + 'Video/Training/Videos/Actor_\"\u001b[0m\n\u001b[0m                                                  ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m EOL while scanning string literal\n"
     ]
    }
   ],
   "source": [
    "dataset = np.empty((0, shape_x*shape_y + 1))\n",
    "\n",
    "j=1\n",
    "i=1\n",
    "k=1\n",
    "\n",
    "for i in range(1,25) :\n",
    "    folder = local_path + 'Datas/Actor_\" + str(i) + \"/\"\n",
    "    onlyfiles = [f for f in os.listdir(folder) if os.path.isfile(os.path.join(folder, f))]\n",
    "\n",
    "    for vid in onlyfiles :\n",
    "    \n",
    "        vidcap = cv2.VideoCapture(folder + vid)\n",
    "        #success,img = vidcap.read()\n",
    "        success = True\n",
    "    \n",
    "        while success :\n",
    "\n",
    "            success,img = vidcap.read()\n",
    "            if not success :\n",
    "                break\n",
    "\n",
    "            gray, detected_faces = detect_face(img)\n",
    "        \n",
    "            for face in detected_faces :\n",
    "            \n",
    "                extracted_face = extract_face_features(gray, face, (0.075, 0.05)) #(0.075, 0.05)\n",
    "                extracted_face = np.append(extracted_face.flatten(), int(vid[6:8]))\n",
    "                dataset = np.vstack((dataset, extracted_face))\n",
    "        \n",
    "        print('Video : '+ str(j))  \n",
    "        j = j + 1\n",
    "        \n",
    "    i = i + 1\n",
    "    \n",
    "np.save(local_path + 'savedmodels/datas', dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train, Test, Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T16:15:59.611879Z",
     "start_time": "2018-12-06T16:15:58.683674Z"
    }
   },
   "outputs": [],
   "source": [
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.regularizers import l2#, activity_l2\n",
    "from keras.optimizers import SGD, RMSprop\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "dataset = np.load(\"/Users/maelfabien/Desktop/LocalDB/Videos/savedmodels/datas.npy\")\n",
    "\n",
    "np.random.seed(2222)  # for reproducibility\n",
    "\n",
    "#Load the scaled data, both pixels and labels\n",
    "X = dataset[:,:-1]\n",
    "y = dataset[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T16:15:59.809341Z",
     "start_time": "2018-12-06T16:15:59.799690Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " [0. 0. 0. ... 1. 0. 0.]\n",
      " ...\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]\n",
      " [1. 0. 0. ... 0. 0. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# binary encode\n",
    "onehot_encoder = OneHotEncoder(sparse=False)\n",
    "y = y.reshape(len(y), 1)\n",
    "y = onehot_encoder.fit_transform(y)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T16:16:01.231003Z",
     "start_time": "2018-12-06T16:16:00.835223Z"
    }
   },
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.35, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T15:42:34.570460Z",
     "start_time": "2018-12-06T15:42:34.563899Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(314, 30375)"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T16:16:28.073232Z",
     "start_time": "2018-12-06T16:16:28.063891Z"
    }
   },
   "outputs": [],
   "source": [
    "#reshape the given pixels into 48 X 48 images\n",
    "X_train = X_train.reshape(X_train.shape[0] ,shape_x , shape_y,1)\n",
    "X_test = X_test.reshape(X_test.shape[0] ,shape_x , shape_y,1)\n",
    "\n",
    "#convert labels to one-hot-encoding\n",
    "Y_tr_labels = np_utils.to_categorical(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## CNN, Tensorboard"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T14:50:11.166647Z",
     "start_time": "2018-11-29T14:50:11.146633Z"
    }
   },
   "source": [
    "<img src = \"/Users/maelfabien/filrouge_pole_emploi/Video/Resources/Schema_Emocognizer.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T15:42:38.257088Z",
     "start_time": "2018-12-06T15:42:38.174231Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:5: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (3, 3), input_shape=(225, 135,..., padding=\"valid\")`\n",
      "  \"\"\"\n",
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:12: UserWarning: Update your `Dense` call to the Keras 2 API: `Dense(128, kernel_initializer=\"lecun_uniform\")`\n",
      "  if sys.path[0] == '':\n"
     ]
    }
   ],
   "source": [
    "#define the model 32 filters in first convolution layer followed by a max pooling and dense layer with dropout (50%)\n",
    "model = Sequential()\n",
    "\n",
    "#nb_filter, nb_row, nb_col\n",
    "model.add(Convolution2D(32, 3, 3, border_mode='valid', input_shape=(shape_x,shape_y,1)))\n",
    "model.add(Activation('relu'))\n",
    "#maxpool\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "#flatten\n",
    "model.add(Flatten())\n",
    "#regular densely-connected NN layer, 128 units = dimension of the output space\n",
    "model.add(Dense(128,init='lecun_uniform'))\n",
    "#randomly selected neurons are ignored during training\n",
    "model.add(Dropout(0.4))\n",
    "model.add(Activation('relu'))\n",
    "#another layer\n",
    "model.add(Dense(8))\n",
    "model.add(Activation('softmax'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T15:42:40.108601Z",
     "start_time": "2018-12-06T15:42:40.097558Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_2 (Conv2D)            (None, 223, 133, 32)      320       \n",
      "_________________________________________________________________\n",
      "activation_4 (Activation)    (None, 223, 133, 32)      0         \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 111, 66, 32)       0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 234432)            0         \n",
      "_________________________________________________________________\n",
      "dense_3 (Dense)              (None, 128)               30007424  \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "activation_5 (Activation)    (None, 128)               0         \n",
      "_________________________________________________________________\n",
      "dense_4 (Dense)              (None, 8)                 1032      \n",
      "_________________________________________________________________\n",
      "activation_6 (Activation)    (None, 8)                 0         \n",
      "=================================================================\n",
      "Total params: 30,008,776\n",
      "Trainable params: 30,008,776\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T15:42:41.538386Z",
     "start_time": "2018-12-06T15:42:41.514568Z"
    }
   },
   "outputs": [],
   "source": [
    "#training the model with cross sgd and nesterov momentum\n",
    "sgd = SGD(lr=0.055, decay=1e-6, momentum=0.9, nesterov=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T15:42:42.139334Z",
     "start_time": "2018-12-06T15:42:42.135343Z"
    }
   },
   "outputs": [],
   "source": [
    "#tensorboard --logdir=logs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T15:56:49.919866Z",
     "start_time": "2018-12-06T15:54:40.777342Z"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/lib/python3.6/site-packages/ipykernel_launcher.py:3: UserWarning: The `nb_epoch` argument in `fit` has been renamed `epochs`.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 581 samples, validate on 314 samples\n",
      "Epoch 1/10\n",
      "581/581 [==============================] - 15s 26ms/step - loss: 2.0023 - acc: 0.2806 - val_loss: 1.9250 - val_acc: 0.3376\n",
      "Epoch 2/10\n",
      "581/581 [==============================] - 12s 20ms/step - loss: 1.8006 - acc: 0.3528 - val_loss: 1.5889 - val_acc: 0.3376\n",
      "Epoch 3/10\n",
      "581/581 [==============================] - 12s 21ms/step - loss: 1.8281 - acc: 0.3133 - val_loss: 1.6182 - val_acc: 0.5796\n",
      "Epoch 4/10\n",
      "581/581 [==============================] - 13s 22ms/step - loss: 1.5314 - acc: 0.4974 - val_loss: 1.5246 - val_acc: 0.4172\n",
      "Epoch 5/10\n",
      "581/581 [==============================] - 13s 22ms/step - loss: 1.5714 - acc: 0.4647 - val_loss: 0.8750 - val_acc: 0.7229\n",
      "Epoch 6/10\n",
      "581/581 [==============================] - 13s 23ms/step - loss: 0.8615 - acc: 0.6971 - val_loss: 0.5037 - val_acc: 0.8503\n",
      "Epoch 7/10\n",
      "581/581 [==============================] - 12s 21ms/step - loss: 0.9141 - acc: 0.6867 - val_loss: 0.5678 - val_acc: 0.7643\n",
      "Epoch 8/10\n",
      "581/581 [==============================] - 13s 22ms/step - loss: 0.5802 - acc: 0.7676 - val_loss: 0.3691 - val_acc: 0.8217\n",
      "Epoch 9/10\n",
      "581/581 [==============================] - 13s 22ms/step - loss: 0.4030 - acc: 0.8451 - val_loss: 0.2726 - val_acc: 0.8567\n",
      "Epoch 10/10\n",
      "581/581 [==============================] - 13s 23ms/step - loss: 0.3735 - acc: 0.8589 - val_loss: 0.1690 - val_acc: 0.9522\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0xb240e76d8>"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.compile(optimizer=sgd, loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "tensorboard = TensorBoard(log_dir=\"logs/{}\".format(time()))\n",
    "model.fit(X_train,y_train, validation_data=(X_test, y_test), batch_size=128 , nb_epoch=10, callbacks=[tensorboard])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T16:03:48.949862Z",
     "start_time": "2018-12-06T16:03:48.817191Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2423"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#save the model weights\n",
    "import h5py\n",
    "json_string = model.to_json()\n",
    "model.save_weights(local_path + 'savedmodels/Model.h5')\n",
    "open(local_path + 'savedmodels/Model.json', 'w').write(json_string)\n",
    "#model.save_weights(local_path + 'savedmodels/Emotion_Face_Detection_Model.h5')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T15:59:28.002919Z",
     "start_time": "2018-12-06T15:59:27.580824Z"
    }
   },
   "source": [
    "plt.figure(figsize=(12,8))\n",
    "\n",
    "plt.plot(cnnhistory.history['loss'])\n",
    "plt.plot(cnnhistory.history['val_loss'])\n",
    "plt.title('Video Model loss')\n",
    "plt.ylabel('loss')\n",
    "plt.xlabel('epoch')\n",
    "plt.legend(['train', 'test'], loc='upper left')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Make prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T16:16:36.863967Z",
     "start_time": "2018-12-06T16:16:36.280124Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from disk\n"
     ]
    }
   ],
   "source": [
    "with open(local_path + 'savedmodels/Model.json','r') as f:\n",
    "    json = f.read()\n",
    "loaded_model = model_from_json(json)\n",
    "\n",
    "loaded_model.load_weights(local_path + 'savedmodels/Model.h5')\n",
    "print(\"Loaded model from disk\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T16:16:40.034529Z",
     "start_time": "2018-12-06T16:16:37.885290Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "acc: 98.73%\n"
     ]
    }
   ],
   "source": [
    "# evaluate loaded model on test data\n",
    "loaded_model.compile(loss='binary_crossentropy', optimizer='rmsprop', metrics=['accuracy'])\n",
    "score = loaded_model.evaluate(X_test, y_test, verbose=0)\n",
    "print(\"%s: %.2f%%\" % (loaded_model.metrics_names[1], score[1]*100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T16:16:55.880184Z",
     "start_time": "2018-12-06T16:16:54.025669Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 4 1 5 6 6 6 3 5 5 1 2 3 6 0 4 7 0 5 6 2 3 7 4 7 5 7 5 2 4 5 1 3 3 7 5\n",
      " 3 1 1 5 2 7 1 6 5 5 6 3 0 2 0 6 0 6 7 5 6 3 5 1 2 2 3 3 0 5 2 4 4 6 6 2 0\n",
      " 0 4 1 1 6 3 6 5 6 4 3 4 3 6 0 3 3 4 7 1 5 6 7 6 6 6 1 4 6 0 4 4 7 7 6 3 0\n",
      " 3 4 4 2 0 5 6 6 1 4 3 6 5 1 7 1 5 1 1 6 5 2 1 0 0 6 3 7 5 0 0 6 7 0 2 3 5\n",
      " 5 7 4 4 1 5 5 7 1 5 7 7 6 0 2 6 5 5 2 0 4 6 5 1 1 0 2 0 5 5 5 6 1 1 1 0 5\n",
      " 4 6 5 5 0 7 3 4 5 3 2 4 0 1 4 6 1 4 1 1 4 1 2 7 2 6 6 2 4 1 5 4 3 2 2 5 3\n",
      " 2 6 0 3 7 6 6 5 1 4 3 6 3 4 0 0 1 7 6 2 5 1 4 6 2 7 3 1 5 5 1 5 1 1 0 1 6\n",
      " 5 2 4 6 1 7 2 2 2 2 7 3 1 6 6 0 5 0 3 4 0 5 5 6 2 5 3 5 4 3 0 1 4 1 5 3 5\n",
      " 4 7 5 7 6 0 5 3 3 6 0 5 2 0 3 5 4 5]\n"
     ]
    }
   ],
   "source": [
    "pred = loaded_model.predict_classes(X_test)\n",
    "print(pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-05T21:46:52.672818Z",
     "start_time": "2018-12-05T21:46:52.666012Z"
    }
   },
   "source": [
    "# Video facial emotion recognition extracting facial features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract facial landmarks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-06T16:42:15.199Z"
    }
   },
   "outputs": [],
   "source": [
    "# import the necessary packages\n",
    "from collections import OrderedDict\n",
    "import argparse\n",
    "import dlib\n",
    "import imutils\n",
    "\n",
    "facial_features_cordinates = {}\n",
    "\n",
    "# define a dictionary that maps the indexes of the facial\n",
    "# landmarks to specific face regions\n",
    "FACIAL_LANDMARKS_INDEXES = OrderedDict([\n",
    "    (\"Mouth\", (48, 68)),\n",
    "    (\"Right_Eyebrow\", (17, 22)),\n",
    "    (\"Left_Eyebrow\", (22, 27)),\n",
    "    (\"Right_Eye\", (36, 42)),\n",
    "    (\"Left_Eye\", (42, 48)),\n",
    "    (\"Nose\", (27, 35)),\n",
    "    (\"Jaw\", (0, 17))\n",
    "])\n",
    "\n",
    "\n",
    "def shape_to_numpy_array(shape, dtype=\"int\"):\n",
    "    # initialize the list of (x, y)-coordinates\n",
    "    coordinates = np.zeros((68, 2), dtype=dtype)\n",
    "\n",
    "    # loop over the 68 facial landmarks and convert them\n",
    "    # to a 2-tuple of (x, y)-coordinates\n",
    "    for i in range(0, 68):\n",
    "        coordinates[i] = (shape.part(i).x, shape.part(i).y)\n",
    "\n",
    "    # return the list of (x, y)-coordinates\n",
    "    return coordinates\n",
    "\n",
    "\n",
    "def visualize_facial_landmarks(image, shape, colors=None, alpha=0.75):\n",
    "    # create two copies of the input image -- one for the\n",
    "    # overlay and one for the final output image\n",
    "    overlay = image.copy()\n",
    "    output = image.copy()\n",
    "\n",
    "    # if the colors list is None, initialize it with a unique\n",
    "    # color for each facial landmark region\n",
    "    if colors is None:\n",
    "        colors = [(19, 199, 109), (79, 76, 240), (230, 159, 23),\n",
    "                  (168, 100, 168), (158, 163, 32),\n",
    "                  (163, 38, 32), (180, 42, 220)]\n",
    "\n",
    "    # loop over the facial landmark regions individually\n",
    "    for (i, name) in enumerate(FACIAL_LANDMARKS_INDEXES.keys()):\n",
    "        # grab the (x, y)-coordinates associated with the\n",
    "        # face landmark\n",
    "        (j, k) = FACIAL_LANDMARKS_INDEXES[name]\n",
    "        pts = shape[j:k]\n",
    "        facial_features_cordinates[name] = pts\n",
    "\n",
    "        # check if are supposed to draw the jawline\n",
    "        if name == \"Jaw\":\n",
    "            # since the jawline is a non-enclosed facial region,\n",
    "            # just draw lines between the (x, y)-coordinates\n",
    "            for l in range(1, len(pts)):\n",
    "                ptA = tuple(pts[l - 1])\n",
    "                ptB = tuple(pts[l])\n",
    "                cv2.line(overlay, ptA, ptB, colors[i], 2)\n",
    "\n",
    "        # otherwise, compute the convex hull of the facial\n",
    "        # landmark coordinates points and display it\n",
    "        else:\n",
    "            hull = cv2.convexHull(pts)\n",
    "            cv2.drawContours(overlay, [hull], -1, colors[i], -1)\n",
    "\n",
    "    # apply the transparent overlay\n",
    "    cv2.addWeighted(overlay, alpha, output, 1 - alpha, 0, output)\n",
    "\n",
    "    # return the output image\n",
    "    print(facial_features_cordinates)\n",
    "    return output\n",
    "\n",
    "# initialize dlib's face detector (HOG-based) and then create\n",
    "# the facial landmark predictor\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor('/Users/maelfabien/Desktop/LocalDB/Videos/landmarks/shape_predictor_68_face_landmarks.dat 2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize detected faces on an image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-06T16:11:55.392203Z",
     "start_time": "2018-12-06T16:11:55.216283Z"
    }
   },
   "outputs": [],
   "source": [
    "from random import randint\n",
    "import cv2\n",
    "import sys\n",
    "import os\n",
    "      \n",
    "CASCADE= local_path + 'FaceDetect/Face_cascade.xml'\n",
    "FACE_CASCADE=cv2.CascadeClassifier(CASCADE)\n",
    "\n",
    "def detect_faces(image_path):\n",
    "\n",
    "\timage=cv2.imread(image_path)\n",
    "\timage_grey=cv2.cvtColor(image,cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "\tfaces = FACE_CASCADE.detectMultiScale(image_grey,scaleFactor=1.16,minNeighbors=5,minSize=(25,25),flags=0)\n",
    "\n",
    "\tfor x,y,w,h in faces:\n",
    "\t    sub_img=image[y-10:y+h+10,x-10:x+w+10]\n",
    "\t    os.chdir(\"Extracted\")\n",
    "\t    cv2.imwrite(str(randint(0,10000))+\".jpg\",sub_img)\n",
    "\t    os.chdir(\"../\")\n",
    "\t    cv2.rectangle(image,(x,y),(x+w,y+h),(255, 255,0),2)\n",
    "\n",
    "\tcv2.imshow(\"Faces Found\",image)\n",
    "\tif (cv2.waitKey(0) & 0xFF == ord('q')) or (cv2.waitKey(0) & 0xFF == ord('Q')):\n",
    "\t\tcv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "start_time": "2018-12-06T16:12:57.385Z"
    }
   },
   "outputs": [],
   "source": [
    "detect_faces('/Users/maelfabien/filrouge_pole_emploi/Common/test_samples/trump.jpg')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Live output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the input image, resize it, and convert it to grayscale\n",
    "image = cv2.imread('/Users/maelfabien/filrouge_pole_emploi/Common/test_samples/trump.jpg')\n",
    "image = imutils.resize(image, width=500)\n",
    "gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "# detect faces in the grayscale image\n",
    "rects = detector(gray, 1)\n",
    "\n",
    "# loop over the face detections\n",
    "for (i, rect) in enumerate(rects):\n",
    "    # determine the facial landmarks for the face region, then\n",
    "    # convert the landmark (x, y)-coordinates to a NumPy array\n",
    "    shape = predictor(gray, rect)\n",
    "    shape = shape_to_numpy_array(shape)\n",
    "\n",
    "    output = visualize_facial_landmarks(image, shape)\n",
    "    cv2.imshow(\"Image\", output)\n",
    "    cv2.waitKey(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "import dlib\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def resize(img, width=None, height=None, interpolation=cv2.INTER_AREA):\n",
    "    global ratio\n",
    "    w, h = img.shape\n",
    "\n",
    "    if width is None and height is None:\n",
    "        return img\n",
    "    elif width is None:\n",
    "        ratio = height / h\n",
    "        width = int(w * ratio)\n",
    "        resized = cv2.resize(img, (height, width), interpolation)\n",
    "        return resized\n",
    "    else:\n",
    "        ratio = width / w\n",
    "        height = int(h * ratio)\n",
    "        resized = cv2.resize(img, (height, width), interpolation)\n",
    "        return resized\n",
    "\n",
    "def shape_to_np(shape, dtype=\"int\"):\n",
    "    # initialize the list of (x, y)-coordinates\n",
    "    coords = np.zeros((68, 2), dtype=dtype)\n",
    "\n",
    "    # loop over the 68 facial landmarks and convert them\n",
    "    # to a 2-tuple of (x, y)-coordinates\n",
    "    for i in range(0, 68):\n",
    "        coords[i] = (shape.part(i).x, shape.part(i).y)\n",
    "\n",
    "    # return the list of (x, y)-coordinates\n",
    "    return coords\n",
    "\n",
    "camera = cv2.VideoCapture(0)\n",
    "\n",
    "predictor_path = 'C:\\\\Users\\\\nikgens\\\\Anaconda3\\\\pkgs\\\\dlib-19.4-np111py36_200\\\\info\\\\recipe\\\\shape_predictor_68_face_landmarks.dat'\n",
    "\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "predictor = dlib.shape_predictor(predictor_path)\n",
    "\n",
    "while True:\n",
    "\n",
    "    ret, frame = camera.read()\n",
    "    if ret == False:\n",
    "        print('Failed to capture frame from camera. Check camera index in cv2.VideoCapture(0) \\n')\n",
    "        break\n",
    "\n",
    "    frame_grey = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "    frame_resized = resize(frame_grey, width=120)\n",
    "\n",
    "    # Ask the detector to find the bounding boxes of each face. The 1 in the\n",
    "    # second argument indicates that we should upsample the image 1 time. This\n",
    "    # will make everything bigger and allow us to detect more faces.\n",
    "    dets = detector(frame_resized, 1)\n",
    "    if len(dets) > 0:\n",
    "        for k, d in enumerate(dets):\n",
    "            # determine the facial landmarks for the face region, then\n",
    "            # convert the facial landmark (x, y)-coordinates to a NumPy array\n",
    "            shape = predictor(frame_resized, d)\n",
    "            shape = shape_to_np(shape)\n",
    "\n",
    "            # loop over the (x, y)-coordinates for the facial landmarks\n",
    "            # and draw them on the image\n",
    "            for (x, y) in shape:\n",
    "                cv2.circle(frame, (int(x/ratio), int(y/ratio)), 3, (255, 255, 255), -1)\n",
    "            #cv2.rectangle(frame, (int(d.left()/ratio), int(d.top()/ratio)),(int(d.right()/ratio), int(d.bottom()/ratio)), (0, 255, 0), 1)\n",
    "\n",
    "    cv2.imshow(\"image\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        cv2.destroyAllWindows()\n",
    "        camera.release()\n",
    "        break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Audio"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-27T16:42:57.303878Z",
     "start_time": "2018-11-27T16:42:57.269816Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "b'Dear Madam, dear Sir,  My name is Ma\\xe2\\x80\\x9al Fabien. I have studied in Lausanne, Switzerland for the past 5 years  and graduated earlier this year from a Master in Actuarial Science. Statistics and  computer science have always been my main interests throughout my studies.  Therefore, I decided to join one of France\\xc3\\x95s top engineering school, Telecom Paris  Tech, for an  MS in Big Data, a post-degree program focusing on both quantitative  techniques of machine learning / deep learning, and computer science. I am currently  seeking a six months internship position starting summer 2019, in the \\xc3\\x9eeld of machine  learning.   Parallel to my studies, I have worked for two years as a teaching assistant for Professor  Boris Nikolov in Corporate Finance, and for Professor Nils Soguel in Public Finance. I  had the opportunity to help undergraduate students in problem sets solving and other  course related tasks. These opportunities are offered in Switzerland to students scoring  the maximal grade at an exam.  I have worked for the past six months in a Swiss insurance company called \\xc3\\x87 ! Vaudoise  Insurance ! \\xc3\\x88. During this end-of-studies internship, I worked as a non-life actuary intern  and was in charge of a product review, including data extraction, handling missing  values, building customer clusters, and going through a pricing review and scenario  development. The \\xc3\\x9enal step of my internship was to present the pricing to the  company\\xc3\\x95s management. During this time, I was given the chance to work daily with a  team of talented people, including actuaries, IT engineers, lawyers, marketing experts  and other interns.  Being passionate about data, I am continuously trying to develop a critical thinking. I  constantly work on my creativity and communication skills, and try to get out of my  comfort zone as often as possible. I \\xc3\\x9end extracting information from data and  introducing the customer to new business and operational perspectives highly  motivating.   I have always been keen on collaborative work, being either through university  projects or during my past experiences. The mentorship offered by Splunk would also  allow me to acquire new skills and knowledges from highly quali\\xc3\\x9eed peers.   I do look forward to hearing from you and remain at your disposal for any further  clari\\xc3\\x9ecations,   Ma\\xe2\\x80\\x9al Fabien 7 rue Barrault, 75013 Paris      + 33 7 87 20 40 33       mael.fabien@gmail.comSPLUNK - COVER LETTER '\n"
     ]
    }
   ],
   "source": [
    "import PyPDF2\n",
    "pdf_file = open(\"/Users/maelfabien/Desktop/Stage2019/SPLUNK/Cover_Letter_Maël_Fabien.pdf\", 'rb')\n",
    "read_pdf = PyPDF2.PdfFileReader(pdf_file)\n",
    "number_of_pages = read_pdf.getNumPages()\n",
    "page = read_pdf.getPage(0)\n",
    "page_content = page.extractText()\n",
    "page_content.encode('utf-8')\n",
    "page_content = page_content.replace(\"\\n\", \" \")\n",
    "print(page_content.encode('utf-8'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T15:33:09.303095Z",
     "start_time": "2018-11-29T15:33:09.297970Z"
    }
   },
   "outputs": [],
   "source": [
    "#Grab file\n",
    "file = path + \"/Voice/pyAudioAnalysis/happy.mp4\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T15:53:58.527088Z",
     "start_time": "2018-11-29T15:53:58.500131Z"
    }
   },
   "outputs": [],
   "source": [
    "#Output in a .npy file\n",
    "\n",
    "def process_input(file) :\n",
    "\n",
    "    pred = []\n",
    "\n",
    "    #clip = mp.VideoFileClip(folder + vid)\n",
    "    vidcap = cv2.VideoCapture(file)\n",
    "    #clip = mp.VideoFileClip(folder + vid)\n",
    "    #clip.audio.write_audiofile(\"audio.wav\")\n",
    "\n",
    "    #fig, ax = plt.subplots(1,2,1)\n",
    "    \n",
    "    # Test dataset path\n",
    "    #test_path = \"audio.wav\"\n",
    "\n",
    "    # Predict\n",
    "    #idx_label, prob, _ = aT.fileClassification(test_path, model_path, \"svm\")\n",
    "    \n",
    "    #plt.subplot(2,2,1)\n",
    "    #plt.bar(label_list, prob)\n",
    "\n",
    "    success = True\n",
    "    \n",
    "    while success :\n",
    "\n",
    "        success,img = vidcap.read()\n",
    "    \n",
    "        if not success :\n",
    "            break\n",
    "\n",
    "        gray, detected_faces = detect_face(img)\n",
    "        \n",
    "        for face in detected_faces :\n",
    "            #face_img = face.reshape(gray.shape[0] ,shapex , shapey,1)\n",
    "            extracted_face = extract_face_features(gray, face, (0.075, 0.05)) #(0.075, 0.05)\n",
    "            extracted_face = extracted_face.reshape(1 ,shape_x , shape_y,1)\n",
    "            pred.append(int(model.predict_classes(extracted_face)))\n",
    "            \n",
    "    global res\n",
    "    \n",
    "    #plt.subplot(1,2,2)\n",
    "    plt.hist(pred)\n",
    "    plt.savefig('/output/testplot.png')\n",
    "    return plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T15:54:25.212436Z",
     "start_time": "2018-11-29T15:53:59.756052Z"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "process_input(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Send report when done"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-11-29T16:07:36.204877Z",
     "start_time": "2018-11-29T16:07:34.796496Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(221, b'2.0.0 closing connection k73sm2505912wmd.36 - gsmtp')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import smtplib\n",
    "from email.mime.multipart import MIMEMultipart\n",
    "from email.mime.text import MIMEText\n",
    "from email.mime.base import MIMEBase\n",
    "from email import encoders\n",
    " \n",
    "fromaddr = \"mael.fabien@gmail.com\"\n",
    "toaddr = \"raphael.lederman@wanadoo.fr\"\n",
    " \n",
    "msg = MIMEMultipart()\n",
    " \n",
    "msg['From'] = fromaddr\n",
    "msg['To'] = toaddr\n",
    "msg['Subject'] = \"Prediction ready\"\n",
    " \n",
    "body = \"Prediction is now ready, check result in attaxched file\"\n",
    " \n",
    "msg.attach(MIMEText(body, 'plain'))\n",
    " \n",
    "filename = \"/output/testplot.png\"\n",
    "attachment = open(filename, \"rb\")\n",
    " \n",
    "part = MIMEBase('application', 'octet-stream')\n",
    "part.set_payload((attachment).read())\n",
    "encoders.encode_base64(part)\n",
    "part.add_header('Content-Disposition', \"attachment; filename= %s\" % filename)\n",
    " \n",
    "msg.attach(part)\n",
    " \n",
    "server = smtplib.SMTP('smtp.gmail.com', 587)\n",
    "server.starttls()\n",
    "server.login(fromaddr, \"*****\")\n",
    "text = msg.as_string()\n",
    "server.sendmail(fromaddr, toaddr, text)\n",
    "server.quit()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
