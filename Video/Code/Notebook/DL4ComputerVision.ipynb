{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Image Classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Dataset In Tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-04T15:14:56.922740Z",
     "start_time": "2018-12-04T15:14:56.233752Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist_data = input_data.read_data_sets('MNIST_data', one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-04T15:15:43.717185Z",
     "start_time": "2018-12-04T15:15:43.713614Z"
    }
   },
   "outputs": [],
   "source": [
    "input_size = 784\n",
    "no_classes = 10\n",
    "batch_size = 100\n",
    "total_batches = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-04T15:16:57.836526Z",
     "start_time": "2018-12-04T15:16:57.825229Z"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "#x_input is the input where the images will be fed later. \n",
    "#y_input is the placeholder where the one-shot labels or targets will be supplied.\n",
    "\n",
    "x_input = tf.placeholder(tf.float32, shape=[None, input_size])\n",
    "y_input = tf.placeholder(tf.float32, shape=[None, no_classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The None in the shape argument indicates that it can be of any size as we have not yet defined the batch size. The second argument is the size of the tensor for x_input and the number of classes for y_input. Based on the type of placeholder, we have sent the data as floats. Next, we can define the perceptron."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The weight variables are initialized with normal random distribution with the shape of input size and number of classes. The input size is 784 here as the image is reshaped into a single vector. The number of classes is 10 which is equal to the number of digits in the dataset. The bias variable is also initialized with random normal distribution with the size equal to the number of classes. The weights and bias are defined as follows:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-04T15:21:40.863094Z",
     "start_time": "2018-12-04T15:21:40.789825Z"
    }
   },
   "outputs": [],
   "source": [
    "weights = tf.Variable(tf.random_normal([input_size, no_classes]))\n",
    "bias = tf.Variable(tf.random_normal([no_classes]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The initialization of the variables can be zeroes but a random normal distribution gives a steady training. The inputs are then weighted and added with the bias to produce logits as shown next:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-04T15:22:17.242462Z",
     "start_time": "2018-12-04T15:22:17.230154Z"
    }
   },
   "outputs": [],
   "source": [
    "logits = tf.matmul(x_input, weights) + bias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The logits produced by the perceptron has to be compared against one-hot labels y_input. The tf.nn.softmax_cross_entropy_with_logits API from TensorFlow does this for us. The loss can be computed by averaging the cross-entropies. Then the cross-entropy is fed through gradient descent optimization done\n",
    "by tf.train.GradientDescentOptimizer. The optimizer takes the loss and minimizes it with a learning rate of 0.5. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-04T15:23:58.310922Z",
     "start_time": "2018-12-04T15:23:58.161835Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-10-475f305446ef>:1: softmax_cross_entropy_with_logits (from tensorflow.python.ops.nn_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "\n",
      "Future major versions of TensorFlow will allow gradients to flow\n",
      "into the labels input on backprop by default.\n",
      "\n",
      "See @{tf.nn.softmax_cross_entropy_with_logits_v2}.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "softmax_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_input, logits=logits)\n",
    "loss_operation = tf.reduce_mean(softmax_cross_entropy)\n",
    "optimiser = tf.train.GradientDescentOptimizer(learning_rate=0.5).minimize(loss_operation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-04T15:24:53.221612Z",
     "start_time": "2018-12-04T15:24:53.000750Z"
    }
   },
   "outputs": [],
   "source": [
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Over a loop, read the data in batches and train the model. Training the model is carried out by running the session with the required tensors. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-04T15:26:09.898373Z",
     "start_time": "2018-12-04T15:26:09.097720Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "13.3438425\n",
      "9.544711\n",
      "8.468291\n",
      "7.0211115\n",
      "5.966028\n",
      "6.1674266\n",
      "6.9207563\n",
      "5.8695483\n",
      "6.3952875\n",
      "6.582772\n",
      "5.7185273\n",
      "5.5500426\n",
      "5.666357\n",
      "4.4614797\n",
      "5.2273693\n",
      "4.0060983\n",
      "4.638402\n",
      "5.0813637\n",
      "3.0039093\n",
      "4.169534\n",
      "4.7116485\n",
      "3.3631196\n",
      "3.7953691\n",
      "3.5868924\n",
      "3.3116663\n",
      "3.4326982\n",
      "2.7202184\n",
      "3.1401513\n",
      "3.2643642\n",
      "3.4223897\n",
      "2.8351662\n",
      "2.6343439\n",
      "3.3724825\n",
      "2.0103126\n",
      "2.8256006\n",
      "2.5645308\n",
      "2.6122391\n",
      "3.1199296\n",
      "2.882144\n",
      "3.4239647\n",
      "1.8731222\n",
      "2.5002882\n",
      "1.9491075\n",
      "2.3378136\n",
      "2.641608\n",
      "2.3660817\n",
      "2.0220249\n",
      "2.2781289\n",
      "1.6602964\n",
      "2.154785\n",
      "1.8930233\n",
      "2.0421805\n",
      "2.2042398\n",
      "2.8818164\n",
      "1.8688518\n",
      "2.548379\n",
      "1.761872\n",
      "2.0188308\n",
      "1.6740566\n",
      "2.2351923\n",
      "1.9016485\n",
      "1.9110863\n",
      "1.8927827\n",
      "1.5942504\n",
      "1.9978166\n",
      "2.1053681\n",
      "1.5109371\n",
      "1.8436159\n",
      "2.1211126\n",
      "1.7985716\n",
      "2.147211\n",
      "2.0203032\n",
      "1.9330999\n",
      "1.6550827\n",
      "2.5906522\n",
      "1.7457494\n",
      "2.0940406\n",
      "1.7813103\n",
      "1.7686172\n",
      "1.5329977\n",
      "1.7671307\n",
      "1.9661092\n",
      "0.9769882\n",
      "2.4779758\n",
      "1.4414487\n",
      "1.6319892\n",
      "1.4561086\n",
      "1.0990819\n",
      "2.053479\n",
      "1.6655915\n",
      "1.0395226\n",
      "1.3257146\n",
      "1.7741635\n",
      "1.0457492\n",
      "1.6963199\n",
      "1.2840796\n",
      "1.4430994\n",
      "1.3676748\n",
      "1.7735982\n",
      "1.7072165\n",
      "1.4904449\n",
      "1.8786098\n",
      "1.1500661\n",
      "1.7425227\n",
      "1.5162314\n",
      "0.9358109\n",
      "1.8381273\n",
      "1.4991822\n",
      "0.85752034\n",
      "1.0792646\n",
      "1.5063213\n",
      "1.7227082\n",
      "1.2394607\n",
      "1.0992413\n",
      "1.9170618\n",
      "1.5759528\n",
      "1.42774\n",
      "1.7021499\n",
      "0.8700685\n",
      "1.1151308\n",
      "1.8933903\n",
      "1.0741687\n",
      "1.3796844\n",
      "1.7471875\n",
      "1.7392595\n",
      "1.4628382\n",
      "1.1490085\n",
      "1.5092431\n",
      "1.1441692\n",
      "1.0250344\n",
      "1.5348067\n",
      "1.1248065\n",
      "1.2409743\n",
      "1.2930651\n",
      "1.10848\n",
      "1.1487235\n",
      "1.2102941\n",
      "1.147341\n",
      "1.2776918\n",
      "0.95338655\n",
      "0.96645486\n",
      "1.8672527\n",
      "1.208394\n",
      "1.4923296\n",
      "1.4177394\n",
      "1.6106218\n",
      "1.2557517\n",
      "1.4431244\n",
      "0.74690646\n",
      "0.59358096\n",
      "1.4591995\n",
      "0.83631027\n",
      "1.1420819\n",
      "1.2508903\n",
      "1.0643831\n",
      "1.7140896\n",
      "0.605586\n",
      "1.2512445\n",
      "1.1776541\n",
      "1.2850137\n",
      "1.4088187\n",
      "0.90865916\n",
      "1.0460714\n",
      "0.90534914\n",
      "1.0297058\n",
      "0.5927991\n",
      "1.1422585\n",
      "1.2716237\n",
      "0.7874104\n",
      "1.0236017\n",
      "1.0201905\n",
      "0.83630824\n",
      "0.9729012\n",
      "1.7001657\n",
      "0.7975766\n",
      "1.0417742\n",
      "2.1055655\n",
      "0.83873284\n",
      "1.2066724\n",
      "0.85128814\n",
      "1.3556833\n",
      "1.0492479\n",
      "0.89063174\n",
      "1.0516206\n",
      "1.1839174\n",
      "1.220011\n",
      "1.2280219\n",
      "0.9382379\n",
      "1.131834\n",
      "0.78303033\n",
      "0.99035573\n",
      "0.65608716\n",
      "0.80066633\n",
      "0.8324752\n",
      "0.8690284\n",
      "1.3948035\n",
      "0.92407\n",
      "0.9042625\n",
      "1.2628328\n",
      "1.4702268\n"
     ]
    }
   ],
   "source": [
    "for batch_no in range(total_batches):\n",
    "    mnist_batch = mnist_data.train.next_batch(batch_size)\n",
    "    _, loss_value = session.run([optimiser, loss_operation], feed_dict={\n",
    "        x_input: mnist_batch[0],\n",
    "        y_input: mnist_batch[1]\n",
    "    })\n",
    "    print(loss_value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-04T15:29:00.721481Z",
     "start_time": "2018-12-04T15:29:00.675396Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy :  0.8087\n"
     ]
    }
   ],
   "source": [
    "predictions = tf.argmax(logits, 1)\n",
    "correct_predictions = tf.equal(predictions, tf.argmax(y_input, 1))\n",
    "accuracy_operation = tf.reduce_mean(tf.cast(correct_predictions,\n",
    "                                               tf.float32))\n",
    "test_images, test_labels = mnist_data.test.images, mnist_data.test.labels\n",
    "accuracy_value = session.run(accuracy_operation, feed_dict={\n",
    "    x_input: test_images,\n",
    "    y_input: test_labels\n",
    "})\n",
    "print('Accuracy : ', accuracy_value)\n",
    "session.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build a multi-layer perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-04T15:31:22.037269Z",
     "start_time": "2018-12-04T15:31:22.029020Z"
    }
   },
   "outputs": [],
   "source": [
    "def add_variable_summary(tf_variable, summary_name):\n",
    "    with tf.name_scope(summary_name + '_summary'):\n",
    "        mean = tf.reduce_mean(tf_variable)\n",
    "        tf.summary.scalar('Mean', mean)\n",
    "        with tf.name_scope('standard_deviation'):\n",
    "            standard_deviation = tf.sqrt(tf.reduce_mean(\n",
    "                tf.square(tf_variable - mean)))\n",
    "        tf.summary.scalar('StandardDeviation', standard_deviation)\n",
    "        tf.summary.scalar('Maximum', tf.reduce_max(tf_variable))\n",
    "        tf.summary.scalar('Minimum', tf.reduce_min(tf_variable))\n",
    "        tf.summary.histogram('Histogram', tf_variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable summary function writes the summaries of a variable. There are five statistics added to the summaries: mean, standard deviation, maximum, minimum and histogram. Summaries can be either a scalar or a histogram. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unlike the previous model, we will resize the MNIST data into a square and use it like a two-dimensional image. The following is the command to reshape the image into 28 image pixels by 28 image pixels:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-04T15:32:46.528605Z",
     "start_time": "2018-12-04T15:32:46.520755Z"
    }
   },
   "outputs": [],
   "source": [
    "x_input_reshape = tf.reshape(x_input, [-1, 28, 28, 1],\n",
    "       name='input_reshape')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dimension -1 denotes that the batch size can be any number. Note that there is an argument called name that will be reflected in the TensorBoard graph for ease of understanding. We will define a 2D convolution layer where the input, filters, kernels, and activations are defined. This method can be called anywhere for further examples and is useful when the activation function has to have Rectified Linear Unit (ReLU) activation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-04T15:34:06.798658Z",
     "start_time": "2018-12-04T15:34:06.792539Z"
    }
   },
   "outputs": [],
   "source": [
    "def convolution_layer(input_layer, filters, kernel_size=[3, 3],\n",
    "                         activation=tf.nn.relu):\n",
    "    layer = tf.layers.conv2d(\n",
    "           inputs=input_layer,\n",
    "           filters=filters,\n",
    "           kernel_size=kernel_size,\n",
    "           activation=activation,\n",
    "    )\n",
    "    add_variable_summary(layer, 'convolution')\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are default parameters for kernel_size and activation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The summaries are added to the layer within the function and the layer is returned. Whenever the function is\n",
    "called, input_layer has to be passed as a parameter. This definition will make our other code simple and small. In a very similar way, we will define a function for\n",
    "the pooling_layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-04T15:35:21.993087Z",
     "start_time": "2018-12-04T15:35:21.987166Z"
    }
   },
   "outputs": [],
   "source": [
    "def pooling_layer(input_layer, pool_size=[2, 2], strides=2):\n",
    "    layer = tf.layers.max_pooling2d(\n",
    "           inputs=input_layer,\n",
    "           pool_size=pool_size,\n",
    "           strides=strides\n",
    "    )\n",
    "    add_variable_summary(layer, 'pooling')\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-04T15:36:07.064382Z",
     "start_time": "2018-12-04T15:36:07.059944Z"
    }
   },
   "outputs": [],
   "source": [
    "def dense_layer(input_layer, units, activation=tf.nn.relu):\n",
    "    layer = tf.layers.dense(\n",
    "           inputs=input_layer,\n",
    "           units=units,\n",
    "           activation=activation\n",
    "    )\n",
    "    add_variable_summary(layer, 'dense')\n",
    "    return layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another convolution layer can be added to transform the sampled features from the first convolution layer to better features. After pooling, we may reshape the activations to a linear fashion in order to be fed through dense layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-04T15:38:19.751690Z",
     "start_time": "2018-12-04T15:38:19.601782Z"
    }
   },
   "outputs": [],
   "source": [
    "convolution_layer_1 = convolution_layer(x_input_reshape, 64)\n",
    "pooling_layer_1 = pooling_layer(convolution_layer_1)\n",
    "convolution_layer_2 = convolution_layer(pooling_layer_1, 128)\n",
    "pooling_layer_2 = pooling_layer(convolution_layer_2)\n",
    "flattened_pool = tf.reshape(pooling_layer_2, [-1, 5 * 5 * 128],\n",
    "                               name='flattened_pool')\n",
    "dense_layer_bottleneck = dense_layer(flattened_pool, 1024)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-04T15:39:33.022053Z",
     "start_time": "2018-12-04T15:39:32.980027Z"
    }
   },
   "outputs": [],
   "source": [
    "dropout_bool = tf.placeholder(tf.bool)\n",
    "dropout_layer = tf.layers.dropout(\n",
    "           inputs=dense_layer_bottleneck,\n",
    "           rate=0.4,\n",
    "           training=dropout_bool\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dropout layer is fed again to a dense layer, which is called logits. Logits is the final layer with activations leading to the number of classes. The activations will be spiked for a particular class, which is the target class, and can be obtained for a maximum of those 10 activations:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-04T15:40:12.182034Z",
     "start_time": "2018-12-04T15:40:12.134769Z"
    }
   },
   "outputs": [],
   "source": [
    "logits = dense_layer(dropout_layer, no_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now the logits can be passed through the softmax layer followed by the cross-entropy calculation as before. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-04T15:41:25.958226Z",
     "start_time": "2018-12-04T15:41:25.908942Z"
    }
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('loss'):\n",
    "    softmax_cross_entropy = tf.nn.softmax_cross_entropy_with_logits(\n",
    "        labels=y_input, logits=logits)\n",
    "    loss_operation = tf.reduce_mean(softmax_cross_entropy, name='loss')\n",
    "    tf.summary.scalar('loss', loss_operation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This loss function can be optimized with tf.train APIs' methods. Here, we will use the Adamoptimiser. The learning rate need not be defined and works well for most cases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-04T15:42:26.908630Z",
     "start_time": "2018-12-04T15:42:26.548338Z"
    }
   },
   "outputs": [],
   "source": [
    "with tf.name_scope('optimiser'):\n",
    "       optimiser = tf.train.AdamOptimizer().minimize(loss_operation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The accuracy is calculated as before but name scopes are added for correct predictions and accuracy calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-04T15:42:59.086517Z",
     "start_time": "2018-12-04T15:42:59.053663Z"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor 'accuracy_1:0' shape=() dtype=string>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with tf.name_scope('accuracy'):\n",
    "    with tf.name_scope('correct_prediction'):\n",
    "        predictions = tf.argmax(logits, 1)\n",
    "        correct_predictions = tf.equal(predictions, tf.argmax(y_input, 1))\n",
    "        with tf.name_scope('accuracy'):\n",
    "            accuracy_operation = tf.reduce_mean(\n",
    "                tf.cast(correct_predictions, tf.float32))\n",
    "tf.summary.scalar('accuracy', accuracy_operation)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next step is to start the session and initialize the variables as in the previous section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-04T15:43:57.373382Z",
     "start_time": "2018-12-04T15:43:57.019485Z"
    }
   },
   "outputs": [],
   "source": [
    "session = tf.Session()\n",
    "session.run(tf.global_variables_initializer())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-04T15:44:57.184962Z",
     "start_time": "2018-12-04T15:44:57.019159Z"
    }
   },
   "outputs": [],
   "source": [
    "merged_summary_operation = tf.summary.merge_all()\n",
    "train_summary_writer = tf.summary.FileWriter('train', session.graph)\n",
    "test_summary_writer = tf.summary.FileWriter('test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-04T15:54:16.897916Z",
     "start_time": "2018-12-04T15:46:09.640682Z"
    }
   },
   "outputs": [],
   "source": [
    "test_images, test_labels = mnist_data.test.images, mnist_data.test.labels\n",
    "for batch_no in range(total_batches):\n",
    "    mnist_batch = mnist_data.train.next_batch(batch_size)\n",
    "    train_images, train_labels = mnist_batch[0], mnist_batch[1]\n",
    "    _, merged_summary = session.run([optimiser, merged_summary_operation],\n",
    "    feed_dict={\n",
    "           x_input: train_images,\n",
    "           y_input: train_labels,\n",
    "           dropout_bool: True\n",
    "    })\n",
    "    train_summary_writer.add_summary(merged_summary, batch_no)\n",
    "    if batch_no % 10 == 0:\n",
    "        merged_summary, _ = session.run([merged_summary_operation,accuracy_operation], feed_dict={\n",
    "               x_input: test_images,\n",
    "               y_input: test_labels,\n",
    "               dropout_bool: False\n",
    "        })\n",
    "        test_summary_writer.add_summary(merged_summary, batch_no)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src='tf.png'>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## MNIST Dataset In Keras"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-04T16:06:09.004131Z",
     "start_time": "2018-12-04T16:06:08.994845Z"
    }
   },
   "source": [
    "### Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-04T16:13:24.731137Z",
     "start_time": "2018-12-04T16:13:24.725484Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "no_classes = 10\n",
    "epochs = 2\n",
    "image_height, image_width = 28, 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-04T16:13:25.539162Z",
     "start_time": "2018-12-04T16:13:25.138535Z"
    }
   },
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.mnist.load_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reshape the vector into an image format, and define the input dimension for the convolution using the code given:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-04T16:13:26.076736Z",
     "start_time": "2018-12-04T16:13:26.072141Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train = x_train.reshape(x_train.shape[0], image_height,image_width, 1)\n",
    "x_test = x_test.reshape(x_test.shape[0], image_height, image_width, 1)\n",
    "input_shape = (image_height, image_width, 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-04T16:04:19.169558Z",
     "start_time": "2018-12-04T16:04:19.161815Z"
    }
   },
   "source": [
    "Convert to float"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-04T16:13:28.085878Z",
     "start_time": "2018-12-04T16:13:27.902138Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalize :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-04T16:13:30.942080Z",
     "start_time": "2018-12-04T16:13:30.904906Z"
    }
   },
   "outputs": [],
   "source": [
    "x_train /= 255\n",
    "x_test /= 255"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Convert categorical variables to one hot encoders :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-04T16:13:32.359965Z",
     "start_time": "2018-12-04T16:13:32.352724Z"
    }
   },
   "outputs": [],
   "source": [
    "y_train = tf.keras.utils.to_categorical(y_train, no_classes)\n",
    "y_test = tf.keras.utils.to_categorical(y_test, no_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-04T16:13:35.300458Z",
     "start_time": "2018-12-04T16:13:35.074781Z"
    }
   },
   "outputs": [],
   "source": [
    "def simple_cnn(input_shape):\n",
    "    model = tf.keras.models.Sequential()\n",
    "    model.add(tf.keras.layers.Conv2D(\n",
    "           filters=64,\n",
    "           kernel_size=(3, 3),\n",
    "           activation='relu',\n",
    "           input_shape=input_shape\n",
    "    ))\n",
    "    model.add(tf.keras.layers.Conv2D(\n",
    "           filters=128,\n",
    "           kernel_size=(3, 3),\n",
    "           activation='relu'\n",
    "    ))\n",
    "    model.add(tf.keras.layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "    model.add(tf.keras.layers.Dropout(rate=0.3))\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    model.add(tf.keras.layers.Dense(units=1024, activation='relu'))\n",
    "    model.add(tf.keras.layers.Dropout(rate=0.3))\n",
    "    model.add(tf.keras.layers.Dense(units=no_classes,\n",
    "   activation='softmax'))\n",
    "    model.compile(loss=tf.keras.losses.categorical_crossentropy,\n",
    "                     optimizer=tf.keras.optimizers.Adam(),\n",
    "                     metrics=['accuracy'])\n",
    "    return model\n",
    "simple_cnn_model = simple_cnn(input_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-04T16:32:42.979690Z",
     "start_time": "2018-12-04T16:13:36.223955Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "Epoch 2/2\n",
      "Train data loss: 0.02458886965283115\n",
      "Train data accuracy: 0.99195\n"
     ]
    }
   ],
   "source": [
    "simple_cnn_model.fit(x_train, y_train, batch_size, epochs, (x_test,y_test))\n",
    "train_loss, train_accuracy = simple_cnn_model.evaluate(x_train, y_train, verbose=0)\n",
    "print('Train data loss:', train_loss)\n",
    "print('Train data accuracy:', train_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2018-12-04T16:33:07.339947Z",
     "start_time": "2018-12-04T16:32:42.984056Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test data loss: 0.04100198608621722\n",
      "Test data accuracy: 0.9864\n"
     ]
    }
   ],
   "source": [
    "test_loss, test_accuracy = simple_cnn_model.evaluate(\n",
    "       x_test, y_test, verbose=0)\n",
    "print('Test data loss:', test_loss)\n",
    "print('Test data accuracy:', test_accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training a model for cats versus dogs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
